# -*- coding: utf-8 -*-
"""MalwareDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tCeoYQcaqKdIq0a2LO4Nt54UA1zy-yhn

# **Malware Detection**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set()
from sklearn import preprocessing, linear_model, model_selection, neighbors, svm, naive_bayes, metrics
from sklearn.metrics import f1_score

"""# **Exploratory Data Analysis**"""

md_df = pd.read_csv('/content/drive/MyDrive/archive/malware-detection/Kaggle-data.csv', nrows=50000)
md_df.head(5)

md_df = md_df.drop('Unnamed: 57', axis=1)

md_df = md_df.set_index('ID')

md_df = md_df.round(decimals=3)

md_df.isnull().values.any()

md_df.isnull().sum()

md_df.loc[md_df['MajorLinkerVersion'].isnull()]

md_df = md_df.drop(index=14)

md_df.head(5)

md_df.info();

md_df.describe()

md_df['Machine'].unique()



"""# **Model Building**"""

X = md_df.drop(['legitimate', 'md5', 'Machine'], axis=1)
y = md_df['legitimate']

rs = np.random.RandomState(seed=10)
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.4, shuffle=True, random_state=rs)
X_test, X_valid, y_test, y_valid = model_selection.train_test_split(X_test, y_test, test_size = 0.5, shuffle=True, random_state=rs)

print(X_test.shape)
print(X_valid.shape)
print(X_train.shape)

clf = neighbors.KNeighborsClassifier()
clf.fit(X_train, y_train)
predictions = clf.predict(X_valid)
metrics.accuracy_score(y_valid, predictions)

classifiers = [neighbors.KNeighborsClassifier(),
               naive_bayes.GaussianNB(),
               naive_bayes.MultinomialNB(),
               linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999),
               svm.LinearSVC(max_iter=9999),
               svm.SVC(kernel='rbf', gamma='auto', max_iter=9999)]
classifier_names = ['KNN','Gaussian NB','Multinomial NB','Logistic','Linear SVM','Non-Linear SVM']
accuracies = []
macro_f1 = []
for clf, name in zip(classifiers,classifier_names):
    clf.fit(X_train, y_train)
    predictions = clf.predict(X_valid)
    acc = metrics.accuracy_score(predictions, y_valid)
    f1 = f1_score(y_valid, predictions, average='macro')
    accuracies.append(acc)
    macro_f1.append(f1)
models = pd.DataFrame({'model':classifier_names, 'accuracy':accuracies, 'macro f1':macro_f1})    
models

"""# **Normal Scaling**"""

scalerminmax = preprocessing.MinMaxScaler()
scalerminmax.fit(X_train)
X_train_scaled_minmax = scalerminmax.transform(X_train)
X_train_scaled_minmax[0]

classifiers = [neighbors.KNeighborsClassifier(),
               naive_bayes.GaussianNB(),
               naive_bayes.MultinomialNB(),
               linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999),
               svm.LinearSVC(max_iter=9999),
               svm.SVC(kernel='rbf', gamma='auto', max_iter=9999)]
classifier_names = ['KNN','Gaussian NB','Logistic','Linear SVM','Non-Linear SVM']
X_valid_scaled_normal = scalerminmax.transform(X_valid)
accuracies_normal = []
macrof1_normal = []
for clf in classifiers:
    if isinstance(clf,naive_bayes.MultinomialNB):
        accuracies_normal.append('N/A')
        macrof1_normal.append('N/A')
    else:
        clf.fit(X_train_scaled_minmax, y_train)
        predictions = clf.predict(X_valid_scaled_normal)
        acc = metrics.accuracy_score(predictions, y_valid)
        accuracies_normal.append(acc)
        f1 = f1_score(y_valid, predictions, average='macro')
        macrof1_normal.append(f1)

models['normalised acc'] = accuracies_normal
models['normalised f1'] = macrof1_normal
models

"""# **Standard Scaling**"""

scaler = preprocessing.StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled[0]

classifiers = [neighbors.KNeighborsClassifier(),
               naive_bayes.GaussianNB(),
               naive_bayes.MultinomialNB(),
               linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999),
               svm.LinearSVC(max_iter=9999),
               svm.SVC(kernel='rbf', gamma='auto', max_iter=9999)]
classifier_names = ['KNN','Gaussian NB','Logistic','Linear SVM','Non-Linear SVM']
X_valid_scaled_standard = scaler.transform(X_valid)
accuracies = []
macrof1_standard = []
for clf in classifiers:
    if isinstance(clf,naive_bayes.MultinomialNB):
        accuracies.append('N/A')
        macrof1_standard.append('N/A')
    else:
        clf.fit(X_train_scaled, y_train)
        predictions = clf.predict(X_valid_scaled_standard)
        acc = metrics.accuracy_score(predictions, y_valid)
        accuracies.append(acc)
        f1 = f1_score(y_valid, predictions, average='macro')
        macrof1_standard.append(f1)
models['standardised acc'] = accuracies
models['normalised acc'] = accuracies_normal
models['standardised f1'] = macrof1_standard
models['normalised f1'] = macrof1_normal
models

# set up a heatmap to visualise the predictions of the logistic regression algorithm
cm = metrics.confusion_matrix(y_valid, predictions)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',cbar=False, ax=ax)
ax.set_xticklabels(['Malware','Legitimate'])
ax.set_yticklabels(['Malware','Legitimate'])
ax.set_xlabel('Predicted Class')
ax.set_title('Heatmap Showing the Predictions for Malware Files.')
# I had to offset the ylim because matplotlib's newest update has caused them to go wonky otherwise
ax.set_ylim([0,2])
ax.set_ylabel('True Class');

"""# Parameter Tuning"""

# Parameter tuning for the KNN baseline model.

best = 0.0
best_macro = 0.0
best_n = 0
best_weight = ''
best_p = 0
best_leaf = 0
best_algorithm = ''
for n in range(3,30):
    for weight in ['uniform','distance']:
        for p in [1,2,3,4,5,6,7,8,9]:
          for leaf in range(1,60):
            for algorithms in ['ball_tree', 'kd_tree', 'brute']:
                clf = neighbors.KNeighborsClassifier(n_neighbors=n, weights=weight, metric='minkowski', p=p, leaf_size=leaf, algorithm=algorithms)
                clf.fit(X_train, y_train)
                # set up the predictions
                prediction = clf.predict_proba(X_valid)
                prediction_int = prediction[:,1] >= 0.3
                prediction_int = prediction_int.astype(np.int)
                acc = metrics.accuracy_score(y_valid, prediction_int)
                macro = f1_score(y_valid, prediction_int, average='macro')
                # find the best parameters for the algorithm 
                if macro > best_macro:
                    best = acc
                    best_n = n
                    best_weight = weight
                    best_p = p
                    best_leaf = leaf
                    best_algorithm = algorithms
                if acc > best:
                    best = acc
print('Best Macro F1 was '+str(best_macro)+' using '+str(best_n)+' neighbours, '+str(best_weight)+' weighting, '+str(best_leaf)+' leaf size, the '+str(best_algorithm)+' algorithm and p='+str(best_p))

"""# Artifical Neural Networks"""

from tensorflow import keras
from sklearn.model_selection import cross_val_score, KFold
from keras.wrappers.scikit_learn import KerasRegressor
from keras.layers import Dense, Dropout, LSTM
from keras.models import Sequential
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
import matplotlib.pyplot as plt

feature_names = [
          "md5",
          "Machine",                   
          "SizeOfOptionalHeader",         
          "Characteristics",           
          "MajorLinkerVersion",        
          "MinorLinkerVersion",    
          "SizeOfCode",        
          "SizeOfInitializedData",    
          "SizeOfUninitializedData",      
          "AddressOfEntryPoint",     
          "BaseOfCode",         
          "BaseOfData",                  
          "ImageBase",                
          "SectionAlignment",         
          "FileAlignment",        
          "MajorOperatingSystemVersion",  
          "MinorOperatingSystemVersion", 
          "MajorImageVersion",   
          "MinorImageVersion",       
          "MajorSubsystemVersion",  
          "MinorSubsystemVersion",   
          "SizeOfImage",    
          "SizeOfHeaders",     
          "CheckSum",         
          "Subsystem",  
          "DllCharacteristics",  
          "SizeOfStackReserve",  
          "SizeOfStackCommit", 
          "SizeOfHeapReserve",   
          "SizeOfHeapCommit",    
          "LoaderFlags",        
          "NumberOfRvaAndSizes",       
          "SectionsNb",    
          "SectionsMeanEntropy",        
          "SectionsMinEntropy",    
          "SectionsMaxEntropy",   
          "SectionsMeanRawsize",  
          "SectionsMinRawsize",  
          "SectionMaxRawsize",     
          "SectionsMeanVirtualsize",  
          "SectionsMinVirtualsize",
          "SectionMaxVirtualsize",
          "ImportsNbDLL",       
          "ImportsNb",           
          "ImportsNbOrdinal",
          "ExportNb",
          "ResourcesNb",
          "ResourcesMeanEntropy",
          "ResourcesMinEntropy",
          "ResourcesMaxEntropy",
          "ResourcesMeanSize",        
          "ResourcesMinSize",          
          "ResourcesMaxSize",           
          "LoadConfigurationSize",  
          "VersionInformationSize",
          "legitimate"
]

features = md_df[feature_names].drop(['legitimate', 'md5', 'Machine'], axis=1).values
legit = md_df['legitimate'].values

md_df['legitimate'].value_counts().plot.bar()
plt.show()

scaler = StandardScaler().fit(features)
mal_features = scaler.transform(features)

def base_model():
    model = Sequential()
    model.add(Dense(1024, input_dim=53, kernel_initializer='normal', activation='relu'))
    model.add(Dense(512, kernel_initializer='normal', activation='relu'))
    model.add(Dense(256, kernel_initializer='normal', activation='relu'))
    #model.add(Dense(128, kernel_initializer='normal', activation='relu'))
    model.add(Dense(1))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

models = []
estimator = KerasRegressor(build_fn=base_model,
                           nb_epoch=50, verbose=0)
models.append(('NeuralNet', estimator))
models.append(('DecisionTree', DecisionTreeRegressor()))
models.append(('RandomForest', RandomForestRegressor(n_estimators=100)))
models.append(('GradienBoost', GradientBoostingRegressor()))
models.append(('SVR', SVR(gamma='auto')))

for name, model in models[:5]:
    kfold = KFold(n_splits=10, random_state=43, shuffle=True)
    results = np.sqrt(-1 * cross_val_score(model, mal_features,
                                           legit, scoring='neg_mean_squared_error', cv=kfold))
    print("{}: {}".format(name, results.mean()))