# -*- coding: utf-8 -*-
"""GMTraffic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U19L85LwqBUUftAIUJOwdy0Cp21ig69V
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set()
from sklearn import preprocessing, linear_model, model_selection, neighbors, svm, naive_bayes, metrics

"""# Pilot Study - Exploratory Data Analysis"""

tfgm_df = pd.read_csv('tfgm Accident Casualty Severity 2018 Balanced.csv')
tfgm_df.head()

tfgm_df.info();

len(tfgm_df['Accident Index'].unique())

tfgm_df.describe()

tfgm_df['LocalAuthority'].unique()

tameside = tfgm_df.loc[tfgm_df['LocalAuthority'] ==110]

len(tameside)

tfgm_df.duplicated(subset='Accident Index', keep='first')

tfgm_df.loc[tfgm_df['Accident Index'] == 1.024340e+11]

tfgm_df.loc[tfgm_df['Accident Index'] == 1.144900e+11]

tfgm_df.loc[tfgm_df['NumberCasualties'] ==13]

night_df = tfgm_df[(tfgm_df['OutputTime'] >= '00:01') & (tfgm_df['OutputTime'] <= '05:59')]
morning_df = tfgm_df[(tfgm_df['OutputTime'] >= '06:00') & (tfgm_df['OutputTime'] <= '11:59')]
afternoon_df = tfgm_df[(tfgm_df['OutputTime'] >= '12:00') & (tfgm_df['OutputTime'] <= '17:59')]
evening_df = tfgm_df[(tfgm_df['OutputTime'] >= '18:00') & (tfgm_df['OutputTime'] <= '23:59')]

len(night_df)

len(morning_df)

len(afternoon_df)

len(evening_df)

df_pivot = tfgm_df.pivot_table(index='VehicleType',columns='Severity',values='NumberCasualties', aggfunc='mean', fill_value=0.0)
fig, ax = plt.subplots(figsize=(8,4))
sns.heatmap(df_pivot, ax=ax, annot=True, fmt='.1f', cmap='Greys', linecolor='k', linewidths=1)
ax.set_title('Average Number of Casualties for Each Vehicle Type based on the Severity', fontsize=18)
ax.set_xlabel('Severity', fontsize=16)
ax.set_ylabel('Type of Vehicle', fontsize=16)
ax.set_yticklabels(['Pedal cycle', 'Motorcycle 50cc and under', 'Motorcycle over 50cc - up to 125cc', 
                    'Motorcycle over 125cc - up to 500cc','Motorcycle over 500cc', 'Taxi/Private car hire', 
                    'Car', 'Minibus', 'Bus/Coach', 'Other motor vehicle', 'Light goods vehicle',
                   'Heavy goods vehicle'], fontsize=12, rotation = 0);
ax.set_xticklabels(['Fatal','Serious','Slight'], fontsize=12, rotation = 0);

fig, axs = plt.subplots(2, 2, figsize=(16,8))
sns.countplot(night_df['NumberVehicles'], ax=axs[0,0])
sns.countplot(morning_df['NumberVehicles'], ax=axs[0,1])
sns.countplot(afternoon_df['NumberVehicles'], ax=axs[1,0])
sns.countplot(evening_df['NumberVehicles'], ax=axs[1,1]);
axs[0,0].set_title('Night (00:01 - 05:59)');
axs[0,0].set_xlabel('Number of Vehicles')
axs[0,0].set_ylabel('Number of Incidents')
axs[0,1].set_title('Morning (06:00 - 11:59)');
axs[0,1].set_xlabel('Number of Vehicles')
axs[0,1].set_ylabel('Number of Incidents')
axs[1,0].set_title('Afternoon (12:00 - 17:59)');
axs[1,0].set_xlabel('Number of Vehicles')
axs[1,0].set_ylabel('Number of Incidents')
axs[1,1].set_title('Evening (18:00 - 23:59)');
axs[1,1].set_xlabel('Number of Vehicles')
axs[1,1].set_ylabel('Number of Incidents')
fig.tight_layout();

fig, axs = plt.subplots(2, 2, figsize=(16,8))
sns.countplot(night_df['Severity'], ax=axs[0,0])
sns.countplot(morning_df['Severity'], ax=axs[0,1])
sns.countplot(afternoon_df['Severity'], ax=axs[1,0])
sns.countplot(evening_df['Severity'], ax=axs[1,1]);
axs[0,0].set_title('Night (00:01 - 05:59)');
axs[0,0].set_xlabel('Severity')
axs[0,0].set_ylabel('Number of Incidents')
axs[0,1].set_title('Morning (06:00 - 11:59)');
axs[0,1].set_xlabel('Severity')
axs[0,1].set_ylabel('Number of Incidents')
axs[1,0].set_title('Afternoon (12:00 - 17:59)');
axs[1,0].set_xlabel('Severity')
axs[1,0].set_ylabel('Number of Incidents')
axs[1,1].set_title('Evening (18:00 - 23:59)');
axs[1,1].set_xlabel('Severity')
axs[1,1].set_ylabel('Number of Incidents')
fig.tight_layout();

fig, ax = plt.subplots(figsize=(16,8))
sns.countplot( x='NumberCasualties', data = tfgm_df, hue='NumberVehicles');
ax.set_ylabel('Number of Incidents')
ax.set_xlabel('Number of Casualties')
ax.set_title('Graph showing the number of casualties for the different numbers of vehicles involved with the crash.')
ax.legend(title='Number of Vehicles involved:', loc='upper right');

fig, ax = plt.subplots(figsize=(16,8))
sns.countplot('Day', data=tfgm_df, ax=ax, hue='Severity')
ax.set_ylabel('Number of Incidents')
ax.set_xlabel('Days of the Week')
ax.set_xticklabels(['Monday','Tuesday','Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
ax.set_title('Graph showing the severity of casualties for the different days of the week.')
ax.legend(['Fatal','Serious','Slight'], title='Severity:');

fig, ax = plt.subplots(figsize=(16,8))
sns.countplot('CasualtyClass', data=tfgm_df, ax=ax, hue='Severity')
ax.set_ylabel('Number of Incidents')
ax.set_xlabel('Casualty Class')
ax.set_xticklabels(['Driver/Rider','Passenger','Pedestrian'])
ax.set_title('Graph showing the severity of casualties for the different classes.')
ax.legend(['Fatal','Serious','Slight'], title='Severity:');

fig, ax = plt.subplots(figsize=(16,8))
ax = sns.countplot(y='VehicleType', data=tfgm_df, orient='h')
ax.set_yticklabels(['Pedal cycle', 'Motorcycle 50cc and under', 'Motorcycle over 50cc - up to 125cc', 
                    'Motorcycle over 125cc - up to 500cc','Motorcycle over 500cc', 'Taxi/Private car hire', 
                    'Car', 'Minibus', 'Bus/Coach', 'Other motor vehicle', 'Light goods vehicle',
                   'Heavy goods vehicle'])
ax.set_xlabel('Number of Incidents')
ax.set_title('Graph showing the number of incidents based on the vehicle type.')

len(tfgm_df.loc[tfgm_df['VehicleType'] == 9])

fig, ax = plt.subplots(figsize=(16,16))
ax = sns.countplot(y='Manoeuvre', data=tfgm_df, orient='h', color=None)
ax.set_ylabel('Manoeuvre')
ax.set_xlabel('Number of Incidents')
ax.set_yticklabels(['Reversing', 'Parked', 'Waiting to go but held up', 'Slowing/Stopped', 'Moving off',
                   'U turn', 'Turning left', 'Waiting to turn left', 'Turning right', 'Waiting to turn right',
                   'Changing lane to left', 'Changing lane to right', 'Overtaking moving vehicle on its offside',
                   'Overtaking stationary vehicle on its offside', 'Overtaking on nearside', 'Going ahead left hand bend',
                   'Going ahead right hand bend', 'Going ahead (other)'])
ax.set_title('Graph showing the number of incidents based on the Manoeuvre performed.');

fig, ax = plt.subplots(figsize=(16,8))
ax = sns.countplot(y='LocalAuthority', data=tfgm_df, orient='h')
ax.set_xlabel('Number of Incidents')
ax.set_ylabel('Local Authority')
ax.set_yticklabels(['Bolton', 'Bury', 'Manchester', 'Oldham', 'Rochdale', 'Salford', 'Stockport',
                   'Tameside', 'Trafford', 'Wigan'])
ax.set_title('Graph showing the number of incidents in the different areas of Greater Manchester.');

fig, ax = plt.subplots(figsize=(16,8))
ax = sns.countplot(y='LocalAuthority', data=tfgm_df, orient='h', hue='Severity')
ax.set_xlabel('Number of Incidents')
ax.set_ylabel('Local Authority')
ax.set_yticklabels(['Bolton', 'Bury', 'Manchester', 'Oldham', 'Rochdale', 'Salford', 'Stockport',
                   'Tameside', 'Trafford', 'Wigan'])
ax.set_title('Graph showing the number of incidents in the different areas of Greater Manchester.');
ax.legend(['Fatal','Serious','Slight'], title='Severity:');

fig, ax = plt.subplots(figsize = (16,6))
sns.distplot(tfgm_df['NumberCasualties'], rug=True, ax=ax);

tfgm_df['Severity'].value_counts(normalize=True)

tfgm_df.loc[(tfgm_df['Severity'] == 1) & (tfgm_df['SevereCasualty'] ==0)]

"""# Pilot Study - Model Building

# 1) Prediction for whether the casualty was severe
"""

tfgm_df['SevereCasualty'].value_counts(normalize=True)

tfgm_df['SevereCasualty'].value_counts()

X = tfgm_df.drop(['SevereCasualty', 'OutputDate', 'OutputTime'], axis=1)
y = tfgm_df['SevereCasualty']

rs = np.random.RandomState(seed=10)
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.4, shuffle=True, random_state=rs)
X_test, X_valid, y_test, y_valid = model_selection.train_test_split(X_test, y_test, test_size = 0.5, shuffle=True, random_state=rs)

print(X_test.shape)
print(X_valid.shape)
print(X_train.shape)

clf_knn = neighbors.KNeighborsClassifier()
knn_cross_val_score = model_selection.cross_val_score(clf_knn, X[['Accident Index','Day','Easting','Northing','LocalAuthority', 'Severity', 'VehicleType',
                                       'NumberCasualties', 'PedMovement', 'PedLocation', 'PoliceReported', 'Manoeuvre', 'NumberVehicles',
                                       'CasualtyClass', 'CarPassenger', 'Skidding']],
                                y, cv=16)

knn_cross_val_score

knn_cross_val_score.mean()

clf_lr = linear_model.LogisticRegression(solver='lbfgs')
lr_cross_val_score = model_selection.cross_val_score(clf_lr, X[['Accident Index','Day','Easting','Northing','LocalAuthority', 'Severity', 'VehicleType',
                                       'NumberCasualties', 'PedMovement', 'PedLocation', 'PoliceReported', 'Manoeuvre', 'NumberVehicles',
                                       'CasualtyClass', 'CarPassenger', 'Skidding']],
                                y, cv=16)

lr_cross_val_score

baseline_predictions = [1 for x in y_valid]
metrics.accuracy_score(y_valid, baseline_predictions)

clf = neighbors.KNeighborsClassifier()
clf.fit(X_train, y_train)
predictions = clf.predict(X_valid)
metrics.accuracy_score(y_valid, predictions)

cm = metrics.confusion_matrix(y_valid, predictions, labels = [0, 1])
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',cbar=False, ax=ax)
ax.set_xticklabels(['Severe','Non-Severe'])
ax.set_yticklabels(['Severe','Non-Severe'])
ax.set_xlabel('Predicted Class')
ax.set_ylabel('True Class');

classifiers = [neighbors.KNeighborsClassifier(),
               naive_bayes.GaussianNB(),
               naive_bayes.MultinomialNB(),
               linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999),
               svm.LinearSVC(max_iter=9999),
               svm.SVC(kernel='rbf', gamma='auto', max_iter=9999)]
classifier_names = ['KNN','Gaussian NB','Multinomial NB','Logistic','Linear SVM','Non-Linear SVM']
accuracies = []
for clf, name in zip(classifiers,classifier_names):
    clf.fit(X_train, y_train)
    predictions = clf.predict(X_valid)
    acc = metrics.accuracy_score(predictions, y_valid)
    accuracies.append(acc)
models = pd.DataFrame({'model':classifier_names, 'baseline':accuracies})    
models

"""# Normal scaling"""

scalerminmax = preprocessing.MinMaxScaler()
scalerminmax.fit(X_train)
X_train_scaled_minmax = scalerminmax.transform(X_train)
X_train_scaled_minmax[0]

classifiers = [neighbors.KNeighborsClassifier(),
               naive_bayes.GaussianNB(),
               naive_bayes.MultinomialNB(),
               linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999),
               svm.LinearSVC(max_iter=9999),
               svm.SVC(kernel='rbf', gamma='auto', max_iter=9999)]
classifier_names = ['KNN','Gaussian NB','Logistic','Linear SVM','Non-Linear SVM']
X_valid_scaled_normal = scalerminmax.transform(X_valid)
accuracies_normal = []
for clf in classifiers:
    if isinstance(clf,naive_bayes.MultinomialNB):
        accuracies_normal.append('N/A')
    else:
        clf.fit(X_train_scaled_minmax, y_train)
        predictions = clf.predict(X_valid_scaled_normal)
        acc = metrics.accuracy_score(predictions, y_valid)
        accuracies_normal.append(acc)
models['normalised'] = accuracies_normal
models

"""# Standard scaling"""

scaler = preprocessing.StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled[0]

classifiers = [neighbors.KNeighborsClassifier(),
               naive_bayes.GaussianNB(),
               naive_bayes.MultinomialNB(),
               linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999),
               svm.LinearSVC(max_iter=9999),
               svm.SVC(kernel='rbf', gamma='auto', max_iter=9999)]
classifier_names = ['KNN','Gaussian NB','Logistic','Linear SVM','Non-Linear SVM']
X_valid_scaled_standard = scaler.transform(X_valid)
accuracies = []
for clf in classifiers:
    if isinstance(clf,naive_bayes.MultinomialNB):
        accuracies.append('N/A')
    else:
        clf.fit(X_train_scaled, y_train)
        predictions = clf.predict(X_valid_scaled_standard)
        acc = metrics.accuracy_score(predictions, y_valid)
        accuracies.append(acc)
models['standardised'] = accuracies
models['normalised'] = accuracies_normal
models

"""# Parameter tuning

# KNearest neighbour - standardised
"""

best = 0.0
best_n = 0
best_weight = ''
best_p = 0
for n in range(3,16):
    for weight in ['uniform','distance']:
        for p in [1,2,3,4,5]:
            clf = neighbors.KNeighborsClassifier(n_neighbors=n, weights=weight, metric='minkowski', p=p)
            clf.fit(X_train_scaled, y_train)
            predictions = clf.predict(X_valid_scaled_standard)
            acc = metrics.accuracy_score(y_valid, predictions)
            if acc > best:
                best = acc
                best_n = n
                best_weight = weight
                best_p = p
print('Best Accuracy was '+str(best)+' using '+str(best_n)+' neighbours, '+str(weight)+' weighting and p='+str(p))

clf = neighbors.KNeighborsClassifier(n_neighbors=12, weights='distance', metric='minkowski', p=5)
clf.fit(X_train_scaled, y_train)
X_test_scaled = scaler.transform(X_test)
predictions = clf.predict(X_test_scaled)
acc = metrics.accuracy_score(y_test, predictions)
acc

"""# KNearest neighbour - normalised"""

best = 0.0
best_n = 0
best_weight = ''
best_p = 0
for n in range(3,16):
    for weight in ['uniform','distance']:
        for p in [1,2,3,4,5]:
            clf = neighbors.KNeighborsClassifier(n_neighbors=n, weights=weight, metric='minkowski', p=p)
            clf.fit(X_train_scaled_minmax, y_train)
            predictions = clf.predict(X_valid_scaled_normal)
            acc = metrics.accuracy_score(y_valid, predictions)
            if acc > best:
                best = acc
                best_n = n
                best_weight = weight
                best_p = p
print('Best Accuracy was '+str(best)+' using '+str(best_n)+' neighbours, '+str(weight)+' weighting and p='+str(p))

clf = neighbors.KNeighborsClassifier(n_neighbors=13, weights='distance', metric='minkowski', p=5)
clf.fit(X_train_scaled_minmax, y_train)
X_test_scaled = scaler.transform(X_test)
predictions = clf.predict(X_test_scaled)
acc = metrics.accuracy_score(y_test, predictions)
acc

"""# Logistic - standardised"""

best = 0.0
best_c = 0
best_solver = ''
best_multiclass = ''
for c in range(1,30):
    for solver in ['saga', 'newton-cg','sag','lbfgs']:
        for multiclass in ['ovr', 'multinomial', 'auto']:
                clf = linear_model.LogisticRegression(solver=solver, C=c, multi_class=multiclass, max_iter=9999)
                clf.fit(X_train_scaled, y_train)
                predictions = clf.predict(X_valid_scaled_standard)
                acc = metrics.accuracy_score(y_valid, predictions)
                if acc > best:
                    best = acc
                    best_c = c
                    best_solver = solver
                    best_multiclass = multiclass
print('Best Accuracy was '+str(best)+' using '+str(best_c)+' C, the '+str(solver)+' solver and using the multi class, '+str(multiclass))

"""# Logistic - normalised"""

best = 0.0
best_c = 0
best_solver = ''
best_multiclass = ''
for c in range(3,16):
    for solver in ['newton-cg', 'saga', 'lbfgs','sag']:
        for multiclass in ['ovr', 'multinomial', 'auto']:
            clf = linear_model.LogisticRegression(solver=solver, C=c, multi_class=multiclass, max_iter=9999)
            clf.fit(X_train_scaled_minmax, y_train)
            predictions = clf.predict(X_valid_scaled_normal)
            acc = metrics.accuracy_score(y_valid, predictions)
            if acc > best:
                best = acc
                best_c = c
                best_solver = solver
print('Best Accuracy was '+str(best)+' using '+str(best_c)+' C, the '+str(solver)+' solver and using the multi class, '+str(multiclass))

clf = linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999)
clf.fit(X_train_scaled, y_train)
predictions = clf.predict(X_valid_scaled_standard)
cm = metrics.confusion_matrix(y_valid, predictions, labels = [0, 1])
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',cbar=False, ax=ax)
ax.set_xticklabels(['Severe','Non-Severe'])
ax.set_yticklabels(['Severe','Non-Severe'])
ax.set_xlabel('Predicted Class')
ax.set_ylabel('True Class');

"""# Main Study - Exploratory Data Analysis"""

all_df = pd.read_csv('tfgm Accident Casualty Severity 2018 All.csv')
all_df.head()

len(all_df.loc[all_df['SexOfDriver'] == 1])

len(all_df.loc[all_df['SexOfDriver'] == 2])

all_df.info()

all_df.describe()

all_df.duplicated()

fig, ax = plt.subplots(figsize=(16,8))
ax = sns.countplot(y='WeatherCondition', data=all_df, orient='h')
ax.set_yticklabels(['Fine (no high winds)', 'Raining(no high winds)', 'Snowing(no high winds)',
                   'Fine(high winds)', 'Raining(high winds)', 'Snowing(high winds)', 'Fog/Mist','Other',
                   'Unknown'])
ax.set_xlabel('Number of Incidents')
ax.set_ylabel('Weather Condition')
ax.set_title('Graph showing the number of incidents based on the weather conditions.');

fig, ax = plt.subplots(figsize=(16,8))
ax = sns.countplot(y='RoadSurface', data=all_df, orient='h')
ax.set_yticklabels(['Dry', 'Wet/Damp', 'Snow', 'Frost/Ice', 'Flood'])
ax.set_xlabel('Number of Incidents')
ax.set_ylabel('Road Surface Condition')
ax.set_title('Graph showing the number of incidents based on the road conditions.');

fig, ax = plt.subplots(figsize=(16,8))
ax = sns.countplot(x='SpeedLimit', data=all_df)
ax.set_ylabel('Number of Incidents')
ax.set_xlabel('Speed Limit')
ax.set_title('Graph showing the number of incidents based on the speed limit.');

"""# Main Study - Model Building"""

all_df['SexOfDriver'].value_counts(normalize=True)
## 1: Male - 2: Female - 3:Not Traced

X = all_df.drop(['SexOfDriver', 'OutputDate', 'OutputTime', 'Accident Index'], axis=1)
y = all_df['SexOfDriver']

rs = np.random.RandomState(seed=10)
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.4, shuffle=True, random_state=rs)
X_test, X_valid, y_test, y_valid = model_selection.train_test_split(X_test, y_test, test_size = 0.5, shuffle=True, random_state=rs)

print(X_test.shape)
print(X_valid.shape)
print(X_train.shape)

baseline_predictions = [1 for x in y_valid]
metrics.accuracy_score(y_valid, baseline_predictions)

clf = neighbors.KNeighborsClassifier()
clf.fit(X_train, y_train)
predictions = clf.predict(X_valid)
metrics.accuracy_score(y_valid, predictions)

cm = metrics.confusion_matrix(y_valid, predictions, labels = [1, 2, 3])
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',cbar=False, ax=ax)
ax.set_xticklabels(['Male','Female', 'Not Traced'])
ax.set_yticklabels(['Male','Female', 'Not Traced'])
ax.set_xlabel('Predicted Class')
ax.set_ylabel('True Class');

classifiers = [neighbors.KNeighborsClassifier(),
               naive_bayes.GaussianNB(),
               naive_bayes.MultinomialNB(),
               linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999),
               svm.LinearSVC(max_iter=9999),
               svm.SVC(kernel='rbf', gamma='auto', max_iter=9999)]
classifier_names = ['KNN','Gaussian NB','Multinomial NB','Logistic','Linear SVM','Non-Linear SVM']
accuracies = []
for clf, name in zip(classifiers,classifier_names):
    clf.fit(X_train, y_train)
    predictions = clf.predict(X_valid)
    acc = metrics.accuracy_score(predictions, y_valid)
    accuracies.append(acc)
models = pd.DataFrame({'model':classifier_names, 'baseline':accuracies})    
models

"""# Normal Scaling"""

scalerminmax = preprocessing.MinMaxScaler()
scalerminmax.fit(X_train)
X_train_scaled_minmax = scalerminmax.transform(X_train)
X_train_scaled_minmax[0]

classifiers = [neighbors.KNeighborsClassifier(),
               naive_bayes.GaussianNB(),
               naive_bayes.MultinomialNB(),
               linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999),
               svm.LinearSVC(max_iter=9999),
               svm.SVC(kernel='rbf', gamma='auto', max_iter=9999)]
classifier_names = ['KNN','Gaussian NB','Logistic','Linear SVM','Non-Linear SVM']
X_valid_scaled_normal = scalerminmax.transform(X_valid)
accuracies_normal = []
for clf in classifiers:
    if isinstance(clf,naive_bayes.MultinomialNB):
        accuracies_normal.append('N/A')
    else:
        clf.fit(X_train_scaled_minmax, y_train)
        predictions = clf.predict(X_valid_scaled_normal)
        acc = metrics.accuracy_score(predictions, y_valid)
        accuracies_normal.append(acc)
models['normalised'] = accuracies_normal
models

"""# Standard Scaling"""

scaler = preprocessing.StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled[0]

classifiers = [neighbors.KNeighborsClassifier(),
               naive_bayes.GaussianNB(),
               naive_bayes.MultinomialNB(),
               linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=9999),
               svm.LinearSVC(max_iter=9999),
               svm.SVC(kernel='rbf', gamma='auto', max_iter=9999)]
classifier_names = ['KNN','Gaussian NB','Logistic','Linear SVM','Non-Linear SVM']
X_valid_scaled_standard = scaler.transform(X_valid)
accuracies = []
for clf in classifiers:
    if isinstance(clf,naive_bayes.MultinomialNB):
        accuracies.append('N/A')
    else:
        clf.fit(X_train_scaled, y_train)
        predictions = clf.predict(X_valid_scaled_standard)
        acc = metrics.accuracy_score(predictions, y_valid)
        accuracies.append(acc)
models['standardised'] = accuracies
models['normalised'] = accuracies_normal
models

"""# Parameter Tuning"""

## This portion of parameter tuning takes a long time to load so I also included a comment below
## with the output I would get. They are the results I used in the essay.

"""# KNN Normalised"""

best = 0.0
best_n = 0
best_weight = ''
best_p = 0
for n in range(3,16):
    for weight in ['uniform','distance']:
        for p in [1,2,3,4,5]:
            clf = neighbors.KNeighborsClassifier(n_neighbors=n, weights=weight, metric='minkowski', p=p)
            clf.fit(X_train_scaled_minmax, y_train)
            predictions = clf.predict(X_valid_scaled_normal)
            acc = metrics.accuracy_score(y_valid, predictions)
            if acc > best:
                best = acc
                best_n = n
                best_weight = weight
                best_p = p
print('Best Accuracy was '+str(best)+' using '+str(best_n)+' neighbours, '+str(weight)+' weighting and p='+str(p))

## Best Accuracy was 0.8707753479125249 using 8 neighbours, distance weighting and p=5

"""# KNN Standardised"""

best = 0.0
best_n = 0
best_weight = ''
best_p = 0
for n in range(3,16):
    for weight in ['uniform','distance']:
        for p in [1,2,3,4,5]:
            clf = neighbors.KNeighborsClassifier(n_neighbors=n, weights=weight, metric='minkowski', p=p)
            clf.fit(X_train_scaled, y_train)
            predictions = clf.predict(X_valid_scaled_standard)
            acc = metrics.accuracy_score(y_valid, predictions)
            if acc > best:
                best = acc
                best_n = n
                best_weight = weight
                best_p = p
print('Best Accuracy was '+str(best)+' using '+str(best_n)+' neighbours, '+str(weight)+' weighting and p='+str(p))

## Best Accuracy was 0.856858846918489 using 8 neighbours, distance weighting and p=5

"""# Logistic Standardised"""

best = 0.0
best_c = 0
best_solver = ''
best_multiclass = ''
for c in range(1,30):
    for solver in ['saga', 'newton-cg','sag','lbfgs']:
        for multiclass in ['ovr', 'multinomial', 'auto']:
                clf = linear_model.LogisticRegression(solver=solver, C=c, multi_class=multiclass, max_iter=9999)
                clf.fit(X_train_scaled, y_train)
                predictions = clf.predict(X_valid_scaled_standard)
                acc = metrics.accuracy_score(y_valid, predictions)
                if acc > best:
                    best = acc
                    best_c = c
                    best_solver = solver
                    best_multiclass = multiclass
print('Best Accuracy was '+str(best)+' using '+str(best_c)+' C, the '+str(solver)+' solver and using the multi class, '+str(multiclass))

## Best Accuracy was 0.841948310139165 using 1 C, the lbfgs solver and using the multi class, auto

"""# Logistic Normalised"""

best = 0.0
best_c = 0
best_solver = ''
best_multiclass = ''
for c in range(3,16):
    for solver in ['newton-cg', 'saga', 'sag','lbfgs']:
        for multiclass in ['ovr', 'multinomial', 'auto']:
            clf = linear_model.LogisticRegression(solver=solver, C=c, multi_class=multiclass, max_iter=9999)
            clf.fit(X_train_scaled_minmax, y_train)
            predictions = clf.predict(X_valid_scaled_normal)
            acc = metrics.accuracy_score(y_valid, predictions)
            if acc > best:
                best = acc
                best_c = c
                best_solver = solver
print('Best Accuracy was '+str(best)+' using '+str(best_c)+' C, the '+str(solver)+' solver and using the multi class, '+str(multiclass))

## Best Accuracy was 0.8449304174950298 using 3 C, the lbfgs solver and using the multi class, auto

"""# Non-Linear SVM Standardised"""

best = 0.0
best_c = 0
best_degree = 0.0
best_gamma = ''
for c in range(1,30):
    for degree1 in range(1,30):
        for gamma1 in ['scale', 'auto']:
                clf = svm.SVC(degree=degree1, C=c, gamma=gamma1, max_iter=9999)
                clf.fit(X_train_scaled, y_train)
                predictions = clf.predict(X_valid_scaled_standard)
                acc = metrics.accuracy_score(y_valid, predictions)
                if acc > best:
                    best = acc
                    best_c = c
                    best_degree = degree1
                    best_gamma = gamma1
print('Best Accuracy was '+str(best)+' using '+str(best_c)+' C, the '+str(gamma1)+' gamma and using the degree = '+str(degree1))

## Best Accuracy was 0.8767395626242545 using 2 C, the auto gamma and using the degree = 29

"""# Non-Linear SVM Normalised"""

import warnings
warnings.filterwarnings("ignore")
best = 0.0
best_c = 0
best_degree = 0.0
best_gamma = ''
for c in range(1,30):
    for degree1 in range(1,30):
        for gamma1 in ['scale', 'auto']:
                clf = svm.SVC(degree=degree1, C=c, gamma=gamma1, max_iter=9999)
                clf.fit(X_train_scaled_minmax, y_train)
                predictions = clf.predict(X_valid_scaled_normal)
                acc = metrics.accuracy_score(y_valid, predictions)
                if acc > best:
                    best = acc
                    best_c = c
                    best_degree = degree1
                    best_gamma = gamma1
print('Best Accuracy was '+str(best)+' using '+str(best_c)+' C, the '+str(gamma1)+' gamma and using the degree = '+str(degree1))

##Best Accuracy was 0.8777335984095428 using 3 C, the auto gamma and using the degree = 29

## The confusion matric below can be run without running the parameter tuning as I manually input the best input parameters
## so you won't have to wait for the parameter tuning portion to load

clf = svm.SVC(degree=29, C=3, gamma='auto', max_iter=9999)
clf.fit(X_train_scaled_minmax, y_train)
predictions = clf.predict(X_valid_scaled_normal)
cm = metrics.confusion_matrix(y_valid, predictions, labels = [1, 2, 3])
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',cbar=False, ax=ax)
ax.set_xticklabels(['Male','Female', 'Not Traced'])
ax.set_yticklabels(['Male','Female', 'Not Traced'])
ax.set_xlabel('Predicted Class')
ax.set_ylabel('True Class');

